#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
We explicitly derive the joint distribution of a linear Gaussian
latent state space model.

The latent process
Z[1], Z[2], ..., Z[T]
in â„Ë¡ is governed by the state model
Z[i] | Z[i-1] ~ N(Z[i-1]*A, Î“) for i = 2, ..., T
with initialisation Z[1] ~ N(m, S)
and the observed latent states
X[1], X[2], ..., X[T]
in â„áµˆ are generated using the measurement model
X[i] | Z[i] ~ N(Z[i]*H, Î›) for i = 1, ..., T

As the resulting joint distribution (Z[1], ..., Z[T]; X[1], ..., X[T])
is multivariate Gaussian, this enables us to calculate marginal
distributions when we encounter hidden variables or missing data.
"""

import warnings

import numba as nb
import numpy as np
import scipy.stats as sp_stats

warnings.simplefilter("ignore")


@nb.jit(
    nb.float64[:, :](
        nb.int64,
        nb.float64[:, :],
        nb.float64[:, :],
        nb.float64[:, :],
    ),
    nopython=True,
    parallel=True,
)
def _CZZii(i: int, S: np.array, A: np.array, Î“: np.array) -> np.array:
    """Covariance matrix for the ith hidden random variable (1-indexed)

    Parameters
    ----------
    i
        index 1<=i<=T
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance

    Returns
    -------
    Var(Z[i])
        dÃ—d symmetric positive semi-definite matrix

    """
    if i == 1:
        return S
    return Î“ + A.T @ _CZZii(i - 1, S, A, Î“) @ A


@nb.jit(
    nb.float64[:, :](
        nb.int64,
        nb.int64,
        nb.float64[:, :],
        nb.float64[:, :],
        nb.float64[:, :],
    ),
    nopython=True,
    parallel=True,
)
def _CZZij(i: int, j: int, S: np.array, A: np.array, Î“: np.array) -> np.array:
    """Covariance between the ith and jth hidden random variables (1-indexed)

    Parameters
    ----------
    i
        index 1<=i<=T
    j
        index 1<=j<=T
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance

    Returns
    -------
    Cov(Z[i], Z[j])
        dÃ—d matrix

    """
    if i == j:
        return _CZZii(i, S, A, Î“)
    elif j > i:
        return _CZZii(i, S, A, Î“) @ np.linalg.matrix_power(A, j - i)
    else:  # j < i
        return _CZZij(j, i, S, A, Î“).T


def CZZ(T: int, S: np.array, A: np.array, Î“: np.array) -> np.array:
    """Covariance for the full hidden autoregressive process

    Parameters
    ----------
    T
        integer number of time steps
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance

    Returns
    -------
    Var(Z)
        dTÃ—dT matrix

    """
    return np.block(
        [
            [_CZZij(i, j, S, A, Î“) for j in range(1, T + 1)]
            for i in range(1, T + 1)
        ]
    )


def _CZX(
    T: int, S: np.array, A: np.array, Î“: np.array, H: np.array
) -> np.array:
    """Covariance between the full hidden and observed processes
    Z & X, respectively

    Parameters
    ----------
    T
        integer number of time steps
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance
    H
        measurement model coefficients

    Returns
    -------
    Cov(X, Z)
        dTÃ—â„“T matrix

    """
    return np.block(
        [
            [_CZZij(i, j, S, A, Î“) @ H for j in range(1, T + 1)]
            for i in range(1, T + 1)
        ]
    )


@nb.jit(
    nb.float64[:, :](
        nb.int64,
        nb.int64,
        nb.float64[:, :],
        nb.float64[:, :],
        nb.float64[:, :],
        nb.float64[:, :],
        nb.float64[:, :],
    ),
    nopython=True,
    parallel=True,
)
def _CXXij(
    i: int,
    j: int,
    S: np.array,
    A: np.array,
    Î“: np.array,
    H: np.array,
    Î›: np.array,
) -> np.array:
    """Covariance between the ith and jth observed random variables
    (1-indexed)

    Parameters
    ----------
    i
        index 1<=i<=T
    j
        index 1<=j<=T
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance
    H
        measurement model coefficients
    Î›
        measurement model covariance

    Returns
    -------
    Cov(X[i], X[j])
        â„“Ã—â„“ matrix

    """
    if i == j:
        return Î› + H.T @ _CZZii(i, S, A, Î“) @ H
    elif j > i:  # i!=j
        return H.T @ _CZZij(i, j, S, A, Î“) @ H
    else:
        return _CXXij(j, i, S, A, Î“, H, Î›).T


def CXX(
    T: int, S: np.array, A: np.array, Î“: np.array, H: np.array, Î›: np.array
) -> np.array:
    """Covariance over all observed random variables

    Parameters
    ----------
    T
        integer number of time steps
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance
    H
        measurement model coefficients
    Î›
        measurement model covariance

    Returns
    -------
    Var(X)
        â„“TÃ—â„“T matrix

    """
    return np.block(
        [
            [_CXXij(i, j, S, A, Î“, H, Î›) for j in range(1, T + 1)]
            for i in range(1, T + 1)
        ]
    )


def CC(
    T: int, S: np.array, A: np.array, Î“: np.array, H: np.array, Î›: np.array
) -> np.array:
    """Full covariance matrix for the joint distribution (Z,X)

    Parameters
    ----------
    T
        integer number of time steps
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance
    H
        measurement model coefficients
    Î›
        measurement model covariance

    Returns
    -------
    Var([Z,X])
        (d+â„“)TÃ—(d+â„“)T matrix

    """
    S, A, Î“, H, Î› = map(np.atleast_2d, (S, A, Î“, H, Î›))
    return np.block(
        [
            [CZZ(T, S, A, Î“), _CZX(T, S, A, Î“, H)],
            [_CZX(T, S, A, Î“, H).T, CXX(T, S, A, Î“, H, Î›)],
        ]
    )


def mmZ(T: int, m: np.array, A: np.array) -> np.array:
    """Full mean vector for the latent process Z

    Parameters
    ----------
    T
        integer number of time steps
    m
        mean of initial hidden random variable
    A
        state space model coefficients

    Returns
    -------
    ð”¼(Z)
        dT-length vector

    """
    return np.hstack(
        [m @ np.linalg.matrix_power(A, i) for i in range(T)]
    ).ravel()


def mmX(T: int, m: np.array, A: np.array, H: np.array) -> np.array:
    """Full mean vector for the observed process X

    Parameters
    ----------
    T
        integer number of time steps
    m
        mean of initial hidden random variable
    A
        state space model coefficients
    H
        measurement model coefficients

    Returns
    -------
    ð”¼(X)
        â„“T-length vector

    """
    return np.hstack(
        [[m @ np.linalg.matrix_power(A, i) @ H] for i in range(T)]
    ).ravel()


def mm(T: int, m: np.array, A: np.array, H: np.array) -> np.array:
    """Full mean vector for the joint distribution (Z, X)

    Parameters
    ----------
    T
        integer number of time steps
    m
        mean of initial hidden random variable
    A
        state space model coefficients
    H
        measurement model coefficients

    Returns
    -------
    ð”¼([Z,X])
        (d+â„“)T-length vector

    """
    A, H = map(np.atleast_2d, (A, H))
    m = np.atleast_1d(m)
    return np.hstack([mmZ(T, m, A), mmX(T, m, A, H)]).ravel()


def full_log_prob(
    z: np.array,
    x: np.array,
    T: np.array,
    m: np.array,
    S: np.array,
    A: np.array,
    Î“: np.array,
    H: np.array,
    Î›: np.array,
) -> np.array:
    """log of joint distribution function (p.d.f.) for (Z,X) calculated using
    our analytically calculated mean and variance functions

    Parameters
    ----------
    z
        TÃ—nÃ—d array of hidden states where
            T length of trajectories
            n number of trajectories
            d dimensionality of the latent space
    x
        TÃ—nÃ—â„“ array of observed variables  where
            T length of trajectories
            n number of trajectories
            â„“ dimensionality of the latent space
    T
        integer number of time steps
    m
        mean of initial hidden random variable
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance
    H
        measurement model coefficients
    Î›
        measurement model covariance

    Returns
    -------
    n-dimensional vector
        log of joint (Gaussian) distribution of the (Z,X) evaluated at (z,x)

    See Also
    --------
    mm
        the mean function we use here
    CC
        the covariance function we use here

    """
    z, x = map(np.atleast_3d, (z, x))
    return sp_stats.multivariate_normal(
        mean=mm(T, m, A, H),
        cov=CC(T, S, A, Î“, H, Î›),
        allow_singular=True,
    ).logpdf(np.hstack((*z[:], *x[:])))


def composite_log_prob(
    z: np.array,
    x: np.array,
    T: int,
    m: np.array,
    S: np.array,
    A: np.array,
    Î“: np.array,
    H: np.array,
    Î›: np.array,
) -> np.array:
    """log of joint distribution function (p.d.f.) for (Z,X) calculated using
    the generation process

    Parameters
    ----------
    z
        TÃ—nÃ—d array of hidden states where
            T length of trajectories
            n number of trajectories
            d dimensionality of the latent space
    x
        TÃ—nÃ—â„“ array of observed variables where
            T length of trajectories
            n number of trajectories
            â„“ dimensionality of the latent space
    T
        integer number of time steps
    m
        mean of initial hidden random variable
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance
    H
        measurement model coefficients
    Î›
        measurement model covariance

    Returns
    -------
    n-dimensional vector
        log of joint (Gaussian) distribution of (Z,X) evaluated at (z,x)

    See Also
    --------
    full_log_prob
        a different way to calculate this quantity

    """
    z, x = map(np.atleast_3d, (z, x))
    S, A, Î“, H, Î› = map(np.atleast_2d, (S, A, Î“, H, Î›))
    log_likelihoods = sp_stats.multivariate_normal(
        mean=m,
        cov=S,
        allow_singular=True,
    ).logpdf(z[0, :, :])
    for t in range(T - 1):
        log_likelihoods += sp_stats.multivariate_normal(
            cov=Î“, allow_singular=True
        ).logpdf(z[t + 1, :, :] - z[t, :, :] @ A)
    for t in range(T):
        log_likelihoods += sp_stats.multivariate_normal(
            cov=Î›, allow_singular=True
        ).logpdf(x[t, :, :] - z[t, :, :] @ H)
    return log_likelihoods


def hidden_log_prob(
    z: np.array,
    T: np.array,
    m: np.array,
    S: np.array,
    A: np.array,
    Î“: np.array,
) -> np.array:
    """log of distribution of Z evaluated at z evaluated using the calculations
    we've made

    Parameters
    ----------
    z
        TÃ—nÃ—d array of hidden states
            T length of trajectories
            n number of trajectories
            d dimensionality of the latent space
    T
        integer number of time steps
    m
        mean of initial hidden random variable
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance

    Returns
    -------
    n-dimensional vector
        log of (Gaussian) distribution of Z evaluated at z

    """
    z = np.atleast_3d(z)
    S, A, Î“ = map(np.atleast_2d, (S, A, Î“))
    return sp_stats.multivariate_normal(
        mean=mmZ(T, m, A),
        cov=CZZ(T, S, A, Î“),
        allow_singular=True,
    ).logpdf(np.hstack(z[:]))


def composite_hidden_log_prob(
    z: np.array,
    T: np.array,
    m: np.array,
    S: np.array,
    A: np.array,
    Î“: np.array,
) -> np.array:
    """log of distribution of Z evaluated at z calculated using the generation
    process

    Parameters
    ----------
    z
        TÃ—nÃ—d array of hidden states where
            T length of trajectories
            n number of trajectories
            d dimensionality of the latent space
    T
        integer number of time steps
    m
        mean of initial hidden random variable
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance

    Returns
    -------
    n-dimensional vector
        of the log of (Gaussian) distribution of Z evaluated at z

    See Also
    --------
    hidden_log_prob
        different way to calculate this quantity

    """
    z = np.atleast_3d(z)
    S, A, Î“ = map(np.atleast_2d, (S, A, Î“))
    log_likelihoods = sp_stats.multivariate_normal(
        mean=m,
        cov=S,
        allow_singular=True,
    ).logpdf(z[0, :, :])
    for t in range(T - 1):
        log_likelihoods += sp_stats.multivariate_normal(
            cov=Î“, allow_singular=True
        ).logpdf(z[t + 1, :, :] - z[t, :, :] @ A)
    return log_likelihoods


def observed_log_prob(
    x: np.array,
    T: np.array,
    m: np.array,
    S: np.array,
    A: np.array,
    Î“: np.array,
    H: np.array,
    Î›: np.array,
) -> np.array:
    """log of joint distribution function (p.d.f.) for X calculated using
    our analytically calculated mean and variance functions

    Parameters
    ----------
    x
        TÃ—nÃ—â„“ array of observed variables where
            T length of trajectories
            n number of trajectories
            â„“ dimensionality of the latent space
    T
        integer number of time steps
    m
        mean of initial hidden random variable
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance
    H
        measurement model coefficients
    Î›
        measurement model covariance

    Returns
    -------
        n-dimensional vector of the log of joint (Gaussian) distribution of
        X evaluated at x

    See Also
    --------
    mmX
        the mean function we use here
    CXX
        the covariance function we use here

    """
    x = np.atleast_3d(x)
    S, A, Î“, H, Î› = map(np.atleast_2d, (S, A, Î“, H, Î›))
    return sp_stats.multivariate_normal(
        mean=mmX(T, m, A, H),
        cov=CXX(T, S, A, Î“, H, Î›),
        allow_singular=True,
    ).logpdf(np.hstack(x[:]))


def full_marginalizable_log_prob(
    z: np.array,
    x: np.array,
    T: np.array,
    m: np.array,
    S: np.array,
    A: np.array,
    Î“: np.array,
    H: np.array,
    Î›: np.array,
) -> np.array:
    """log of joint distribution function (p.d.f.) for (Z,X),
    marginalising over missing dimensions of the data

    Parameters
    ----------
    z
        TÃ—nÃ—d array of hidden states, potentially including np.nan's, where
            T length of trajectories
            n number of trajectories
            d dimensionality of the latent space
    x
        TÃ—nÃ—â„“ array of observed variables, potentially including np.nan's,
        where
            T length of trajectories
            n number of trajectories
            â„“ dimensionality of the latent space
    T
        integer number of time steps
    m
        mean of initial hidden random variable
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance
    H
        measurement model coefficients
    Î›
        measurement model covariance

    Returns
    -------
    log of joint (Gaussian) distribution of (Z,X) evaluated at (z,x)
        non-finite dimensions have been marginalized out

    See Also
    --------
    mm
        the mean function we use here
    CC
        the covariance function we use here

    """
    z, x = map(np.atleast_3d, (z, x))
    zx = np.ma.masked_invalid(np.hstack((*z[:], *x[:])))
    p = np.zeros(zx.shape[0])

    # compute mean and covariance once
    mean = mm(T, m, A, H)
    cov = CC(T, S, A, Î“, H, Î›)

    # loop over data and calculate for each instance
    for i in range(zx.shape[0]):
        zx_i = zx[i, :]
        p[i] = sp_stats.multivariate_normal(
            mean=mean[~zx_i.mask],
            cov=cov[~zx_i.mask, :][:, ~zx_i.mask],
            allow_singular=True,
        ).logpdf(zx_i.compressed())
    return p


@nb.guvectorize(
    [
        (
            nb.float64[:, :],
            nb.float64[:],
            nb.float64[:, :],
            nb.float64[:],
        )
    ],
    "(n,d),(d),(d,d)->(n)",
    nopython=True,
    # fastmath=True,
)
def multivariate_normal_log_likelihood(
    x: np.array, Î¼: np.array, Î£: np.array, p: np.array
):
    """computes the log likelihood of a multivariate N(Î¼,Î£) distribution
    evaluated at the rows of x and assigns it to p;
    if x[i], 1<=i<=n, contains np.nan's or np.inf's for some elements
    (anything that is not finite), it marginalizes these out of the computation

    Parameters
    ----------
    x
        nÃ—d-dimensional matrix containing rows of observations
    Î¼
        d-dimensional vector
    Î£
        dÃ—d-dimensional covariance matrix
    p
        n-dimensional vector containing the log-likelihoods of interest

    Returns
    -------
        assigns p[i] to be the log likelihood of X~N(Î¼,Î£) at X=x[i,:]

    """
    x = np.atleast_2d(x)
    Î£ = np.atleast_2d(Î£)

    for i in range(p.size):
        m = x[i, :].ravel() - Î¼.ravel()
        idx = np.argwhere(np.isfinite(m)).ravel()
        p[i] = -0.5 * np.log(
            (2 * np.pi) ** idx.size * np.linalg.det(Î£[idx, :][:, idx])
        ) - 0.5 * m[idx] @ np.linalg.solve(Î£[idx, :][:, idx], m[idx])


def sample_trajectory(
    n: int,
    T: int,
    m: np.array,
    S: np.array,
    A: np.array,
    Î“: np.array,
    H: np.array,
    Î›: np.array,
    rng: np.random.Generator = np.random.default_rng(42),
) -> tuple[np.array, np.array]:
    """Given model parameters, this function creates n samples of (Z,X)

    Parameters
    ----------
    n
        integer number of samples to create
    T
        integer number of time steps
    m
        mean of initial hidden random variable
    S
        covariance of initial hidden random variable
    A
        state space model coefficients
    Î“
        state space model covariance
    H
        measurement model coefficients
    Î›
        measurement model covariance
    rng
        random number generator

    Returns
    -------
    a tuple (z,x)
        n samples from from the joint distribution of (Z,X)
        with provided parameters
        z has shape TÃ—nÃ—d
        x has shape TÃ—nÃ—â„“
    """
    S, A, Î“, H, Î› = map(np.atleast_2d, (S, A, Î“, H, Î›))

    z = np.zeros(shape=(T, n, m.shape[0]))
    x = np.zeros(shape=(T, n, H.shape[1]))

    z[0, :, :] = sp_stats.multivariate_normal(mean=m, cov=S).rvs(
        size=n, random_state=rng
    )
    x[0, :, :] = z[0, :, :] @ H + sp_stats.multivariate_normal(cov=Î›).rvs(
        size=n, random_state=rng
    )
    for t in range(T - 1):
        z[t + 1, :, :] = z[t, :, :] @ A + sp_stats.multivariate_normal(
            cov=Î“,
        ).rvs(size=n, random_state=rng)
        x[t + 1, :, :] = z[t + 1, :, :] @ H + sp_stats.multivariate_normal(
            cov=Î›
        ).rvs(size=n, random_state=rng)
    return z, x


def sample_nonlinear_nongaussian_trajectory(
    n: int,
    dz: int,
    dx: int,
    T: int,
    m: callable,
    f: callable,
    Î“: callable,
    h: callable,
    Î›: callable,
    rng: np.random.Generator = np.random.default_rng(42),
) -> tuple[np.array, np.array]:
    """Given model parameters, this function creates n samples of (Z,X)

    Parameters
    ----------
    n
        integer number of samples to create
    dz
        dimensionality of hidden / latent states
    dx
        dimensionality of measurements / observations
    T
        integer number of time steps
    m
        sampler for initial hidden random variable (taking size, rng as input)
        e.g. lambda size, rng: sp_stats.multivariate_normal(cov=Î›).rvs(
            size=size, random_state=rng
        )
    f
        function for state space model
        e.g. lambda z: z ** 2
    Î“
        noise part of state space model (taking size, rng as input)
    h
        function for measurement model
        e.g. lambda z: np.sin(z)
    Î›
        noise part of measurement model (taking size, rng as input)
    rng
        random number generator

    Returns
    -------
    a tuple (z,x)
        n samples from from the joint distribution of (Z,X)
        with provided parameters
        z has shape TÃ—nÃ—d
        x has shape TÃ—nÃ—â„“
    """

    z = np.zeros(shape=(T, n, dz))
    x = np.zeros(shape=(T, n, dx))

    z[0, :, :] = m(n, rng)
    x[0, :, :] = np.apply_along_axis(func1d=h, axis=-1, arr=z[0, :, :]) + Î›(
        n, rng
    )

    for t in range(T - 1):
        z[t + 1, :, :] = np.apply_along_axis(
            func1d=f, axis=-1, arr=z[t, :, :]
        ) + Î“(n, rng)
        x[t + 1, :, :] = np.apply_along_axis(
            func1d=h, axis=-1, arr=z[t + 1, :, :]
        ) + Î›(n, rng)
    return z, x


def marginalizable_gaussian_log_prob(
    x: np.array, Î¼: np.array = None, Î£: np.array = None
):
    """gaussian log probability that marginalizes over np.nan values

    Parameters
    ----------
    x
        nÃ—d-dimensional matrix where rows are observations
    Î¼
        d-dimensional vector; defaults to the zero vector
    Î£
        dÃ—d-dimensional covariance matrix; defaults to the identity

    Returns
    -------
    log_probs
        n-dimensional vector of log(Î·(x[i];Î¼,Î£)) indexed over the rows i
    """
    x = np.atleast_2d(x)
    n, d = x.shape
    if Î¼ is None:
        Î¼ = np.zeros(shape=d)
    if Î£ is None:
        Î£ = np.eye(d)
    else:
        Î£ = np.atleast_2d(Î£)
    xm = np.ma.masked_invalid(x)
    p = np.zeros(n)
    for i in range(n):
        p[i] = sp_stats.multivariate_normal(
            mean=Î¼[~xm[i].mask],
            cov=Î£[~xm[i].mask, :][:, ~xm[i].mask],
            allow_singular=True,
        ).logpdf(xm[i].compressed())
    return p


# run some tests if called as a script
if __name__ == "__main__":
    print("Running tests...")

    # make reproducible
    np.random.seed(42)
    rng = np.random.default_rng(42)

    n = 100000
    T = 10
    d_hidden = 5  # d
    d_observed = 3  # â„“

    # randomly initialize model coefficients
    A = rng.normal(scale=0.5, size=(d_hidden, d_hidden))
    Î“ = np.eye(d_hidden) / 2.0
    H = rng.normal(size=(d_hidden, d_observed))
    Î› = np.eye(d_observed) / 3.0
    m = rng.normal(size=(d_hidden))
    S = np.eye(d_hidden) / 5.0

    # generate a sample of n trajectories, each of length T
    z, x = sample_trajectory(n, T, m, S, A, Î“, H, Î›)

    # CC() is a positive definite, symmetric matrix
    assert np.alltrue(np.linalg.eig(CC(T, S, A, Î“, H, Î›))[0] > 0)
    assert np.allclose(CC(T, S, A, Î“, H, Î›), CC(T, S, A, Î“, H, Î›).T)
    print("CC() is a valid covariance matrix")

    # the average over the n samples should be close to the analytically
    # calculated mean
    assert np.allclose(
        np.hstack((*z.mean(axis=1)[:], *x.mean(axis=1)[:])),
        mm(T, m, A, H),
        rtol=0.03,
        atol=0.03,
    )
    print("The empirical and analytic estimates for ð”¼([Z,X]) are close")

    assert np.allclose(
        np.cov(np.hstack((*z[:], *x[:])), rowvar=False),
        CC(T, S, A, Î“, H, Î›),
        rtol=0.05,
        atol=0.05,
    )
    print("The empirical and analytic estimates for Var([Z,X]) are close")

    assert np.allclose(
        hidden_log_prob(z, T, m, S, A, Î“),
        composite_hidden_log_prob(z, T, m, S, A, Î“),
    )
    print("The analytic and composite calculations for p(Z=z) are close")

    assert np.allclose(
        full_log_prob(z, x, T, m, S, A, Î“, H, Î›),
        composite_log_prob(z, x, T, m, S, A, Î“, H, Î›),
    )
    print("The analytic and composite calculations for p(Z=z,X=x) are close")

    assert np.allclose(
        full_log_prob(z[:, :100, :], x[:, :100, :], T, m, S, A, Î“, H, Î›),
        full_marginalizable_log_prob(
            z[:, :100, :], x[:, :100, :], T, m, S, A, Î“, H, Î›
        ),
    )

    print(
        "The analytic and marginalizable distributions agree "
        "on fully available data"
    )

    z[1, 0, :] = z[3, 0, :] = z[5, 0, :] = np.nan
    x[2, 0, :] = x[4, 0, :] = x[6, 0, :] = np.nan
    assert np.isfinite(
        full_marginalizable_log_prob(
            z[:, 0:1, :], x[:, 0:1, :], T, m, S, A, Î“, H, Î›
        ).ravel()[0]
    )
    print("The marginalizable distribution works with nan's")

    p = multivariate_normal_log_likelihood(
        z[0, :, :], m, S, np.zeros_like(z[0, :, 0])
    )
    assert np.allclose(
        p,
        np.array(
            sp_stats.multivariate_normal(
                mean=m,
                cov=S,
            ).logpdf(z[0, :, :])
        ),
    )

    print("Our multivariate normal log-pdf agrees with the standard")

    Îž = np.diag(range(1, 4))
    Î¶ = sp_stats.multivariate_normal(cov=Îž).rvs(size=4, random_state=rng)
    Î¶[0, 1] = Î¶[1, 2] = Î¶[3, 2] = np.nan
    Ï€ = multivariate_normal_log_likelihood(
        Î¶, np.zeros(3), Îž, np.zeros_like(Î¶[:, 0])
    )
    assert np.isclose(
        Ï€[0],
        sp_stats.multivariate_normal(
            mean=np.zeros(2),
            cov=np.diag([1, 3]),
        ).logpdf(np.ma.masked_invalid(Î¶[0]).compressed()),
    )

    print("Marginalisation strategy appears consistent")

    # train initial state model, state transition model, and measurement model
    # using kernel density estimation

    import statsmodels.api as sm

    long = False
    state_init_mdl = sm.nonparametric.KDEMultivariate(
        data=z[0, -100:, :],
        var_type="c" * z.shape[-1],
        bw="cv_ml" if long else "normal_reference",
    )
    state_mdl = sm.nonparametric.KDEMultivariateConditional(
        endog=np.row_stack([*z[1:, -100:, :]]),
        exog=np.row_stack([*z[:-1, -100:, :]]),
        dep_type="c" * z.shape[-1],
        indep_type="c" * z.shape[-1],
        bw="cv_ml" if long else "normal_reference",
    )
    measurement_mdl = sm.nonparametric.KDEMultivariateConditional(
        endog=np.row_stack([*x[:, -100:, :]]),
        exog=np.row_stack([*z[:, -100:, :]]),
        dep_type="c" * x.shape[-1],
        indep_type="c" * z.shape[-1],
        bw="cv_ml" if long else "normal_reference",
    )

    log_prob_kde = np.log(state_init_mdl.pdf(z[0, :1000, :]))
    for t in range(T - 1):
        log_prob_kde += np.log(
            state_mdl.pdf(
                endog_predict=z[t + 1, :1000, :], exog_predict=z[t, :1000, :]
            )
        )
    for t in range(T):
        log_prob_kde += np.log(
            measurement_mdl.pdf(
                endog_predict=x[t, :1000, :], exog_predict=z[t, :1000, :]
            )
        )

    assert (
        sm.OLS(
            full_log_prob(
                z[:, 1:1000, :], x[:, 1:1000, :], T, m, S, A, Î“, H, Î›
            ),
            log_prob_kde[1:],
        )
        .fit()
        .rsquared
        > 0.99
    )

    print("KDE prototyping completed")

    w2 = sp_stats.multivariate_normal().rvs(size=(10, 2), random_state=rng)
    w3 = np.column_stack((w2, np.nan * np.ones(shape=(10,))))
    w4 = np.column_stack((w3, np.nan * np.ones(shape=(10,))))

    np.testing.assert_allclose(
        marginalizable_gaussian_log_prob(w2),
        marginalizable_gaussian_log_prob(w3),
    )

    np.testing.assert_allclose(
        marginalizable_gaussian_log_prob(w3),
        marginalizable_gaussian_log_prob(w4),
    )

    np.testing.assert_allclose(
        multivariate_normal_log_likelihood(
            w4,
            np.zeros(shape=(w4.shape[1])),
            np.eye(w4.shape[1]),
            np.zeros(shape=(w4.shape[0])),
        ),
        marginalizable_gaussian_log_prob(w4),
    )

    print("Marginalisation testing completed")

    # generate a sample of n nonlinear non-gaussian trajectories

    m0 = lambda size, rng: sp_stats.multivariate_normal(mean=m, cov=S).rvs(
        size=size, random_state=rng
    )

    f0 = lambda z: z @ A
    Î“0 = lambda size, rng: sp_stats.multivariate_normal(cov=Î“).rvs(
        size=size, random_state=rng
    )
    h0 = lambda z: z @ H
    Î›0 = lambda size, rng: sp_stats.multivariate_normal(cov=Î›).rvs(
        size=size, random_state=rng
    )
    z0, x0 = sample_nonlinear_nongaussian_trajectory(
        n, d_hidden, d_observed, T, m0, f0, Î“0, h0, Î›0
    )

    # we added some nan's the original so just compare the ends
    assert np.allclose(z[10:], z0[10:])
    assert np.allclose(x[10:], x0[10:])

    print("Nonlinear sampler agrees with linear sampler on linear model")

    assert np.allclose(
        full_log_prob(
            z[..., 0],
            x[..., 0],
            T,
            m[0],
            S[0, 0],
            A[0, 0],
            Î“[0, 0],
            H[0, 0],
            Î›[0, 0],
        )[10:],
        composite_log_prob(
            z[..., 0],
            x[..., 0],
            T,
            m[0],
            S[0, 0],
            A[0, 0],
            Î“[0, 0],
            H[0, 0],
            Î›[0, 0],
        )[10:],
    )
    print("The 1-d versions are in agreement")

    print("Tests succeeded")
